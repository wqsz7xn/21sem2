* Parallel Programs
** Task, Data and Synchronisation
- Task parallelism: Where you partition tasks carried out in solving the problem
  among the cores and each core carries out more or less similar operations on
  its part of the data
  - Where each marker only marks a single question out of all the questions for
    the papers, cores in this case are carrying out different operations for
    each core
- Data parallelism: Where you have an amount of data and then you split that
  data amongst the cores and they do the whole workload for that data
  - Where each marker marks whole papers but the stack of papers is split
    between all of the markers, everyone in this context is executing roughly
    the same operations
- The last type is synchronisation where there must be sync point in order for
  the algorithms to be working together (because each core works at its own pace), therefore if the master core is making data available first then the other cores can't just race off and start computing before the master core has even put up the data, a sync point must be used
** Types of Memory
- Shared and distributed, where shared each core can share memory while in the
  distributed setting you have to assume that all memory is private to each core
  (or to each cluster of cores) (shared memory within a multi CPU environment)


#+DOWNLOADED: screenshot @ 2021-07-20 18:38:25
[[file:images/Parallel_Programs/2021-07-20_18-38-25_screenshot.png]]

* Parallel Hardware and Parallel Software
- Memory is used to store both program and data instructions
- Program instructions are coded data which tell the computer to do something
- Data is simply information to be used by the program
- A central processing unit (CPU) gets instructions and/or data from memory,
  decodes the instructions and then *sequentially* performs them

#+DOWNLOADED: screenshot @ 2021-07-20 19:26:35
[[file:images/Parallel_Hardware_and_Parallel_Software/2021-07-20_19-26-35_screenshot.png]]

** Cache mappings
- *fully associative cache*: is one in which a new line can be placed at any
  location in the cache
- *direct mapped*: in which each cache line has a unique location in the cache
  to which it will be assigned
- intermediate solutions are called n-way set associative

* Instruction-level Parallelism
- An attempt to improve processor performance by having multiple processor
  components (*functional units*) simultaneously executing instructions. There
  are two types, *pipelining* and *multiple issue*
** Pipelining
- Divide the machinery for the algorithm into different logical blocks, while
  you are computing the last stage of the algorithm for data x, you can already
  be phasing in data y at the start of the block, this means that one piece of
  computation doesn't have to wait until the other is fully done. not k stage ->
  k fold improvement
* Misc
- *Write back scheme*: where you write to a piece of memory and put a bit on it
  to indicate that it is dirty, then when it is evicted from memory the bit is
  then checked and if it's a dirty address then you write it back to disk so
  that it can be updated
