* Lecture 1
- End goal: achieve speedup of k fold when you have k processors, in the real
  world this doesn't happen but we want to be as close to this as possible, this
  is the upperbound limit
** Elements of a Parallel Computer
 - Hardware
   - Multiple processes
   - Multiple memories
   - Interconnection network (ICN)
 - System Software
   - Parallel operating system
   - Programming constructs to express/orchestrate concurrency
 - Application Software
   - Parallel Algorithms
    
** Parallel Computing Platform
 - Logical Organization
   - The user's view of the machine as it is being preseent via its system
     software
 - Physical Organization
   - The actual hardware architecture
 - Physical architecture is to a large extent independent of the logical
   architecture

** Logical Organization Elements
- Control Mechanism
  - SISD/SIMD/MIMID/MISD
    - Single/Multiple Instruction Stream & Single/Multiple Data sream
  - SPMD: single program multiple data

#+DOWNLOADED: screenshot @ 2021-07-21 19:04:34
[[file:images/Lecture_1/2021-07-21_19-04-34_screenshot.png]]

** Logical Organization Elements
- Communication Model
  - Shared-address space
  - Message passing
  - UMA/NUMA/ccNUMA
- These are the ways that our processors are communicating (how they are sharing
  memory)
  #+DOWNLOADED: screenshot @ 2021-07-21 19:06:09
  [[file:images/Lecture_1/2021-07-21_19-06-09_screenshot.png]]
- *A*: where all of the processors are sharing an address space
- *B*: where all of the processors have their own local address space and are
  also sharing an address space for all processors
- *C*: where all processors have their own address space and nothing else,
  communication is from local address space to local address space only

** Physical Organization
- Ideal parallel computer architecture is PRAM (parallel random access machine)
- PRAM models:
  - EREW/ERCW/CREW/CRCW
    - Exclusive/Concurrent Read or and write
    - Basically this is all of the read locks and write locks stuff that you
      have covered from SQL, where write locks and read locks don't coexist, but
      multiple read locks can
  - Concurrent writes are resolved via
    - Common/arbitrary/priority/sum
** Physical Organization
- Interconnection Networks (ICN)
  - Provide processor-to-processor and processor-to-memory connections
  - Networks are classified as the following
*** Static
- Consistent of a number of point to point links
  - direct network
- Historically used to link processor to processor
  - Distributed memory system
*** Dynamic
- The network consisted of switching elements that the various processors attach
  to
  - Indirect network
- Historically used to link processors to memory
  - shared memory systems
** Static and Dynamic ICNs

#+DOWNLOADED: screenshot @ 2021-07-21 19:14:24
[[file:images/Lecture_1/2021-07-21_19-14-24_screenshot.png]]

** Evaluation Metrics for ICNs
- Diameter
  - The maximum distance between any two nodes (smaller the better)
- Connectivity
  - The minimum number of arcs that must be removed to break it into two
    disconnected networks (larger the better) (measures the multiplicity of
    paths)
- Bisection width
  - The minimum number of arcs that must be removed to partition the network into
    two equal halves (larger the better)
- Bisection bandwidth
  - Applies to networks with weighted arcs: weights correspond to the link width
    (how much data it can transfer)
  - The minimum volume of communication allowed between any two halves of a
    network (larger the better)
- Cost
  - The number of links in the network (smaller is better)
- Symmetry

#+DOWNLOADED: screenshot @ 2021-07-21 19:19:16
[[file:images/Lecture_1/2021-07-21_19-19-16_screenshot.png]]

** Network Typologies
- Bus based networks
  - Shared medium
  - information is being broadcasted
  - Evaluation
    - Diameter: O(1)
    - Connectivity: O(1)
    - Bisection width: O(1)
    - Cost: O(p)


#+DOWNLOADED: screenshot @ 2021-07-21 19:21:03
[[file:images/Lecture_1/2021-07-21_19-21-03_screenshot.png]]

- Crossbar Networks
  - Switch-based network
  - Supports simultaneous connections
  - Evaluation:
    - Diameter: O(1)
    - Connectivity: O(1)
    - Bisection width: O(p), you can split it in half and still have half the
      network communicate 
    - Cost: O(p^2)

#+DOWNLOADED: screenshot @ 2021-07-22 13:57:16
[[file:images/Lecture_1/2021-07-22_13-57-16_screenshot.png]]
- Multistage Interconnection Networks

#+DOWNLOADED: screenshot @ 2021-07-22 13:57:38
[[file:images/Lecture_1/2021-07-22_13-57-38_screenshot.png]]
- You have to send through multiple stages in order for processors to
  communicate, the communication time is delayed
- The cost is much lower

- Multistage Switch Architecture

#+DOWNLOADED: screenshot @ 2021-07-22 13:59:30
[[file:images/Lecture_1/2021-07-22_13-59-30_screenshot.png]]
- Far less wires, but your communication from address to address is constrained
  by ==log(n)==
- Connecting the Various Stages

#+DOWNLOADED: screenshot @ 2021-07-22 13:59:45
[[file:images/Lecture_1/2021-07-22_13-59-45_screenshot.png]]

- "The perfect shuffle"
** Blocking in a Multistage Switch
- Routing is done by comparing the bit-level representation of source and
  destination addresses
- match goes via pass-through
- Mismatch goes via cross-over

#+DOWNLOADED: screenshot @ 2021-07-22 14:08:16
[[file:images/Lecture_1/2021-07-22_14-08-16_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-07-22 15:16:44
[[file:images/Lecture_1/2021-07-22_15-16-44_screenshot.png]]
- a) is good if there are no master/slaves
- b) is good for master/slave, there is no need for any of the slaves to
  communicate

* Lecture 2
** Network Topologies

#+DOWNLOADED: screenshot @ 2021-07-23 13:45:32
[[file:images/Lecture_2/2021-07-23_13-45-32_screenshot.png]]

- Here the b) is better for master/slave type layouts while a) is better for a
  type of network that has no masters/slaves where each processor is equal,
  completely completed networks are nice but cost prohibitive

#+DOWNLOADED: screenshot @ 2021-07-26 11:26:22
[[file:images/Lecture_2/2021-07-26_11-26-22_screenshot.png]]

** Hypercubes

#+DOWNLOADED: screenshot @ 2021-07-26 11:26:56
[[file:images/Lecture_2/2021-07-26_11-26-56_screenshot.png]]

- Hypercubes are quite good for parallel computing because they have some nice
  properties that are well balanced (check the table of properties to ensure
  this). Furthermore they have a nice construction heuristic that allows us to
  easily scale them up into multiple dimensions
- Look at hamming distance for hypercubes
** Trees
 - If you have two neighbouring processing nodes then you don't have to travel
   very far, for example if you have two neighbouring processors and then you
   want to get data from one to the other then you would only have to travel two
   links (four links for a round trip). This is very little.
 - Each leaf is a processing node and every internal node is a switching node,
   every switching node combines the links from the children nodes (switches or
   processing nodes) and combines them for the parent
** Summary
- You can tell that a completely connected netowrk is nice but cost prohibitive
** Hypercubes revisted
- These can be generated using Cayley Graphs
** Broadcast (telephone communication)
- Broadcast network, you call your friend and they call their friend, every step
  the number of nodes who know about the information doubles
- Soonest everyone can be informed is thus in log(n) steps (n is the number of nodes)
- Broadcast vs gossip
  - gossip is everyone has information while in broadcast there is one souce of
    information
** Physical Organization
- Cache Coherence is when there is a clash in memory because it's stored in two
  different bits of memory that is different from the main global memory
- Ensure that a local cache is coherent with the value that is in the main
  memory
  - Invalidate and update in order to do this
** Topology Embeddings
- You would use some processors in this system in order to create this mapping
- We want a small dilation????
- Meshes map easily onto hypercubes and that's why they are famous
- Mapping between networks
  - Useful in the early days of parallel computing when tology specific
    algorithms were being developed
- Embedding quality metrics
  - Dilation
    - Maximum number of lines an edge is mapped to
  - Congestion
    - Maximum number of edges mapped on a single link
* Lecture 3
Now that we have covered some mappings and topologies we are going to go through
some algs (sequential)
** Dense Matrix-Vector Multiplication
- ==O(n^2)==, two nested for loops
- hopefully when we are parallelising this then you're going to get a better
  order than n^2


#+DOWNLOADED: screenshot @ 2021-07-26 17:20:39
[[file:images/Lecture_3/2021-07-26_17-20-39_screenshot.png]]

** Dense Matrix-Matrix Multiplication
- Two matrices multiplied which is ==O(n^3)==, remember you can makes this
  better by using divide and conquer

#+DOWNLOADED: screenshot @ 2021-07-26 17:20:49
[[file:images/Lecture_3/2021-07-26_17-20-49_screenshot.png]]

** Sparse Matrix-Vector Multiplication
- Hopefully we can do better than n^2 when we have less than n^2 entries in the
  matrix or vector, in the diagram the black dots are non-zero, obvious if
  something is zero we can skip that multiplication because it's obviously going
  to be zero when we are done with it

#+DOWNLOADED: screenshot @ 2021-07-26 17:21:09
[[file:images/Lecture_3/2021-07-26_17-21-09_screenshot.png]]

** Floyd's All-Pairs Shortest Path
- The shortest path between all pairs of nodes the in the graph. It's a O(n^3),
  you're basically bruting for a shortest path, this is going to be bad but
  there's better ways to do this (like sparse matrix detection)

#+DOWNLOADED: screenshot @ 2021-07-26 17:21:48
[[file:images/Lecture_3/2021-07-26_17-21-48_screenshot.png]]

** Quicksort
- Using element q (picked at random) as a pivot, where everything to the left is
  less than q and everything to the right is greater than q. You then apply this
  recursively to the subparts
- Although this algorithm in the lides might be a bit different, don't be too
  surprised about this

#+DOWNLOADED: screenshot @ 2021-07-26 17:21:57
[[file:images/Lecture_3/2021-07-26_17-21-57_screenshot.png]]

** Minimum Finding
Nothing to note, he talked about FP being able to reduce this though

#+DOWNLOADED: screenshot @ 2021-07-26 17:22:08
[[file:images/Lecture_3/2021-07-26_17-22-08_screenshot.png]]

** 15-Puzzle Problem
- Final goal state is a sorted magic square
- You should look this up, i havne't seen this before
- You want to find the shortest solution (in number of moves) or just a
 solution
- You're just shuffling things around until you get an ordering

#+DOWNLOADED: screenshot @ 2021-07-26 17:22:19
[[file:images/Lecture_3/2021-07-26_17-22-19_screenshot.png]]

** Parallel Algorithm vs Parallel Formulation
- Parallel Formulation
  - Refers to a parallelization of a serial algorithm, you're using the serial
    algorithm and then you're parallelising it
- Parallel Algorithm
  - May represent an entirely different algorithm than that one used serially
- We primarily focus on "Parallel Formulations"
  - Our goal today is to primarily discuss how to develop such parallel
    formulations
  - Of course there will always be examples of "parallel algorithms" that were
    not derived from serial algorithms
** Elements of a Parallel Algorithm/Formulation
- Pieces of work that can be done concurrently
  - tasks
- Mapping of the tasks onto multiple processors
  - Processes vs Processors and how to balance the two
- Distribution of input/output ^ intermediate data across the different
  processors
- Management the access of shared data
  - Either input or intermediate
- Synchronization of the processors at various points of the parallel execution
- The best outcome is k processes decrease the runtime by k-fold
** Finding Concurrent Pieces of Work
- Decomposition
  - The process of dividing the compution into smaller pieces of work, ie *tasks*
    - Note that if you can identify this then you basically have done yourself a
      huge favour because this is the hard part of this, usually when you figure
      out how to do this step then you will have a good idea on how to do the
      rest
- Tasks are programmer dfined and are considered to be indivisible (they are the
  minimum unit), also called the granularity of the  task

** Example: Dense Matrix-Vector Multiplication

#+DOWNLOADED: screenshot @ 2021-07-26 17:22:45
[[file:images/Lecture_3/2021-07-26_17-22-45_screenshot.png]]

Here we can split the large matrix into tasks of three rows such that the
processors do this 4 times faster (holy grail case)

** Example: Query Processing

#+DOWNLOADED: screenshot @ 2021-07-26 17:26:52
[[file:images/Lecture_3/2021-07-26_17-26-52_screenshot.png]]

#+DOWNLOADED: screenshot @ 2021-07-26 17:26:59
[[file:images/Lecture_3/2021-07-26_17-26-59_screenshot.png]]

- The first (left) is better here because the subtasks are not as dependent as
  the one on the right, precedence directed graph

** Task-Dependency Graph 
- In most cases, there are dependencies between the different tasks
  - Certain tasks can only start once some other tasks have finished
    - Eg: producer consumer relationships
- These dependencies are represented using a DAG called a *task-dependency graph*

#+DOWNLOADED: screenshot @ 2021-07-26 17:28:25
[[file:images/Lecture_3/2021-07-26_17-28-25_screenshot.png]]

- Key concepts derived from the task-dependency graph
  - Degree of Concurrency
    - The number of tasks that can be concurrently executed
      - We usually care about the average degree of concurrency
  - Critical Path
    - The longest vertex-weighted path in the graph
      - The weights represent task size, whatever is your longest critical task
        is going to bound your runtime (weakest link mindset), you are trying to
        minimize this
      - Even though they have the same critical path, the first one is
        preferable because it has better concurrency
  - Tasks granularity affects both of the above characteristics
** Task-Interaction Graph
- Captures the pattern of interaction between tasks
  - This graph usually contains the task-dependency graph as a subgraph
    - i.e: there may be interacgtions between tasks even if there are no
      dependencies
      - These interactions usually occur due to accesses on shared data

#+DOWNLOADED: screenshot @ 2021-07-26 15:53:20
[[file:images/Lecture_3/2021-07-26_15-53-20_screenshot.png]]
- You are inserting an edge if there is some interaction between processors in
  order to complete or compose tasks together

** Tasks Dependency/Interaction Graphs
- These graphs are important in developing effectively mapping the tasks onto
  the different processors
  - Maximuze concurrency and minimuze overheads

#+DOWNLOADED: screenshot @ 2021-07-26 15:55:45
[[file:images/Lecture_3/2021-07-26_15-55-45_screenshot.png]]

* Lecture 4
** Common Decomposition Methods
- Data decomposition: Decompose the data to various processors
- Recursive decomposition: Splitting into subproblems
- Exploratory decomposition
- Speculative decomposition
- Hybrid decomposition: Combine decomposition methods to get a better one
** Recursive Decomposition
- Suitable for problems that can be solved using the divide and conquer paradigm
- Each of the /subproblems/ generated by the divide step becomes a task
*** Example: Quicksort

#+DOWNLOADED: screenshot @ 2021-07-28 14:17:20
[[file:images/Lecture_4/2021-07-28_14-17-20_screenshot.png]]

- Make subproblems and pivot (highlighted in grey)
** Example: Finding the Minimum
- Note that we can obtain divide and conquer algorithms for problems that are
  traditionally solved using non-divide nad conquer approaches
- Keep in mind about this the degree of concurrency
- Find min for first half of array and find min of second half of array
- Number of levels is log(n) where n is the number of elements
- You can have all of the level tasks done parallel, unless you run out of
  processors (which is likely because the levels are doubling this requriement
  everytime for every level that you're going down
- 

#+DOWNLOADED: screenshot @ 2021-07-28 14:18:06
[[file:images/Lecture_4/2021-07-28_14-18-06_screenshot.png]]

** Recursive Decomposition
- How good are the decompositions that it produces?
  - Average concurrency?
  - Critical path?
- How do the quicksort and min-finding decompositions measure up?
- Hopefully with recursive routines you're going to have an equal workload which
  is helpful
** Data Decomposition
- Used to derive concurrency for problems that operate on large amounts of data
- The idea is to derive the tasks by focusing on the multiplicity of data
- You're dividing the input
- Data decomposition is often performed in two steps
  - Step 1: Partition the data
  - Step 2: Induce a computational partitioning from teh data partitioning
- Which data should we partition?
  - Input/Output/Intermediate?
    - Well... all of the above--leading to different data decomposition methods
- How do induce a computational partitioning?
  - Owner-computes rule
*** Example: Matrix-Matrix Multiplication
- Partitioning the output data

#+DOWNLOADED: screenshot @ 2021-07-28 14:21:57
[[file:images/Lecture_4/2021-07-28_14-21-57_screenshot.png]]
- Here you have partitioned the arrays into independent tasks that can be more
  quickly processed
- Assume that this is using shared memory (you can assume this for all of
  Michaels part)

** Example Matrix-Matrix Multiplication

#+DOWNLOADED: screenshot @ 2021-07-28 16:20:24
[[file:images/Lecture_4/2021-07-28_16-20-24_screenshot.png]]

- 9, 10 ,11, 12 are blocked until their parent nodes (two parents) are
  completed.
- This is fairly parallel

** Data Decomposition
- Is the most widely used decomposition technique
  - After all paralllel processing is often applied to problems that have a lot
    of data
  - Splitting the work based on this data is the natural way to extract high
    degree of concurrency
- It is used by itself or in conjunction with other decomposition methods
  - Hybrid decomposition

#+DOWNLOADED: screenshot @ 2021-07-28 16:22:49
[[file:images/Lecture_4/2021-07-28_16-22-49_screenshot.png]]

- Use data decomposition in the first phase and switch to recursive
  decomposition in the next phases

** Exploratory Decomposition
- Used to decompose computations that correspond to a search of a space of
  solutions
- You're searching the space but as soon as you find an appropriate answer then
  you're doing to kill to computation. The search space isn't always visited
  symmetrically and can weight itself

*** Example: 15 Puzzle Problem

#+DOWNLOADED: screenshot @ 2021-07-28 16:26:13
[[file:images/Lecture_4/2021-07-28_16-26-13_screenshot.png]]

- Do a branch and bound like above / BFS

** Exploratory Decomposition
- It is not as general purpose, as if you're unlucky you can get a bad runtime =O(n^2)=
- It can result in speedup anomalies
  - engineered slow-down or superlinear speedup

#+DOWNLOADED: screenshot @ 2021-07-28 16:27:04
[[file:images/Lecture_4/2021-07-28_16-27-04_screenshot.png]]

- This is a good image to explain exploratory speedups
- Second case parallel time = serial time but the energy expended is 4x
** Speculative Decomposition
- When you have extra processors doing nothing you don't want them doing
  nothing, you then make them guess what the answer is (using probabilities)
  (for example guess english letters you would guess 'e' because it's the most common)
- Used to extract concurrency in problems in which the /next/ step is one of
  many possible actions taht can only be determined when the current tasks
  finished
- This decomposition assumes a certain /outcome/ of the currently executed task
  and executes some of the next steps
  - Just like speculative execution at the microprocessor level

*** Example: Discrete Event Simulation

#+DOWNLOADED: screenshot @ 2021-07-28 16:38:12
[[file:images/Lecture_4/2021-07-28_16-38-12_screenshot.png]]
- a e g h i would be the critical path
- You could instead guess the output of G and use F, then compute H using the
  guessed value and real value. (Speculative answer). This means there's more
  ouputs doing (somewhat) useful work

** Speculative Execution
- If predictions are wrong
  - Work is wasted
  - work my need to be undone
    - state restoring overhead
      - memory/computations
- However, it may be the only way to extract concurrency!

** Mapping the Tasks
- Why do we care about task mapping?
  - Can I just randomly assign them to the available processors?
- Proper mapping is critical as it needs to minimize the parallel processing
  overheads
  - If T_p is the parallel runtime on /p/ processors and T_s is the serial
    runtime, then the /total overhead/ T_alpha is p*T_p - T_s
    - The work done by the parallel system beyond that required by the serial
      system
  - Overhead sources:

#+DOWNLOADED: screenshot @ 2021-07-28 16:43:09
[[file:images/Lecture_4/2021-07-28_16-43-09_screenshot.png]]

** Why Mapping can be Complicated?
- Proper mapping needs to take into account the task-dependency and interaction
  graphs
  - Are the tasks available a priori?
    - Static vs dynamic task generation
  - How about their computational requirements?
    - Are they uniform of non-uniform?
    - Do we know them a priori?
  - How much data is associated with each task?
  - How about the interaction patterns between the tasks?
    - Are they static or dynamic?
    - Do we know them beforehand?
    - Are they data instance dependent?
    - Are they regular or irregular
    - Are they read-only or read-write?
- Depending on the above characteristics different mapping techniques are
  required of different complexity and cost

*** Example: Simple & Complex Task Interaction

#+DOWNLOADED: screenshot @ 2021-07-28 16:49:38
[[file:images/Lecture_4/2021-07-28_16-49-38_screenshot.png]]

** TODO Up to Sources
* Misc
- OpenMP second assignment
** SIMD
- Single instruction multiple data stream
- the same instruction is execute synchronously by all processing units
** MIMD
- Each processing element is capable of executing a different program
  independent of the other processing elements
- SIMD computers require less hardware the MIMD computers because they have only
  one global control unit. Furthermore, SIMD computers require less memory
  because only one copy of the program needs to be stored. In contracts, MIMD
  computers store the program and operating system at each processor
** Communication Model of Parallel Platforms
** Parallel Programs
*** Task, Data and Synchronisation
 - Task parallelism: Where you partition tasks carried out in solving the problem
   among the cores and each core carries out more or less similar operations on
   its part of the data
   - Where each marker only marks a single question out of all the questions for
     the papers, cores in this case are carrying out different operations for
     each core
 - Data parallelism: Where you have an amount of data and then you split that
   data amongst the cores and they do the whole workload for that data
   - Where each marker marks whole papers but the stack of papers is split
     between all of the markers, everyone in this context is executing roughly
     the same operations
 - The last type is synchronisation where there must be sync point in order for
   the algorithms to be working together (because each core works at its own pace), therefore if the master core is making data available first then the other cores can't just race off and start computing before the master core has even put up the data, a sync point must be used
*** Types of Memory
 - Shared and distributed, where shared each core can share memory while in the
   distributed setting you have to assume that all memory is private to each core
   (or to each cluster of cores) (shared memory within a multi CPU environment)


 #+DOWNLOADED: screenshot @ 2021-07-20 18:38:25
 [[file:images/Parallel_Programs/2021-07-20_18-38-25_screenshot.png]]

** Parallel Hardware and Parallel Software
 - Memory is used to store both program and data instructions
 - Program instructions are coded data which tell the computer to do something
 - Data is simply information to be used by the program
 - A central processing unit (CPU) gets instructions and/or data from memory,
   decodes the instructions and then *sequentially* performs them

 #+DOWNLOADED: screenshot @ 2021-07-20 19:26:35
 [[file:images/Parallel_Hardware_and_Parallel_Software/2021-07-20_19-26-35_screenshot.png]]

*** Cache mappings
 - *fully associative cache*: is one in which a new line can be placed at any
   location in the cache
 - *direct mapped*: in which each cache line has a unique location in the cache
   to which it will be assigned
 - intermediate solutions are called n-way set associative

** Instruction-level Parallelism
 - An attempt to improve processor performance by having multiple processor
   components (*functional units*) simultaneously executing instructions. There
   are two types, *pipelining* and *multiple issue*
*** Pipelining
 - Divide the machinery for the algorithm into different logical blocks, while
   you are computing the last stage of the algorithm for data x, you can already
   be phasing in data y at the start of the block, this means that one piece of
   computation doesn't have to wait until the other is fully done. not k stage ->
   k fold improvement
** Misc
 - *Write back scheme*: where you write to a piece of memory and put a bit on it
   to indicate that it is dirty, then when it is evicted from memory the bit is
   then checked and if it's a dirty address then you write it back to disk so
   that it can be updated
** Arc Connectivity 
A graph is k arc connected if k directed disjoint paths between any
two vertices u and v exist. Below is an example of a two arc connected graph.

#+DOWNLOADED: screenshot @ 2021-07-23 15:06:36
[[file:images/Misc/2021-07-23_15-06-36_screenshot.png]]
** Graph Diameter
The diameter of a graph is the distance of the longest shortest path between any
two nodes
** Bisection Width
When you split the network into two, how many links are you breaking? The
bisection should be done in such a way that the number of links broken is
minimised
